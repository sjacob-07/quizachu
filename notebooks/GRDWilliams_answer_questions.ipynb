{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering questions using Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main solution using pre-made model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/george/Documents/LeWagon/Transformers_Hugging_Face\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Install requirements\"\"\"\n",
    "# Install the transformers library from HuggingFace\n",
    "!pip install transformers torch pytesseract\n",
    "# You'll also need some extra tools that some of these models use under the hood\n",
    "! pip install sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:06:10.871398: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-30 16:06:10.871464: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-11-30 16:06:11.015659: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 16:06:13.054510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-30 16:06:13.054670: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-30 16:06:13.054682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Import packages\"\"\"\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"Import our question answering model\"\"\"\n",
    "question_answerer = pipeline(model = 'deepset/roberta-base-squad2')\n",
    "\n",
    "\"\"\"For web scraping\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scrape BBC as a possible source of context\"\"\"\n",
    "story = \"https://www.bbc.co.uk/news/uk-england-beds-bucks-herts-67407334\" # Example article\n",
    "response = requests.get(story)\n",
    "soup = BeautifulSoup(response.content)\n",
    "article = []\n",
    "for para in soup.find_all(\"div\", {\"data-component\": \"text-block\"}):\n",
    "    article.append(para.text)\n",
    "article = \" \".join(article)\n",
    "article\n",
    "\n",
    "questions = ['Where will profit go?','Who produced the song?','What is the song called?',\\\n",
    "             'Who gave the song its first play?','When will the song be released?','Who wrote the song?',\\\n",
    "             'Where was the video filmed?','How has nala been delighting commuters?'\\\n",
    "             \"Who's pictures went viral?\"] # Example questions for example article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Open a file as a possible source of context\"\"\"\n",
    "file = open(\"quizachu/example_article.txt\", \"r\") # Example file\n",
    "content = file.read()\n",
    "print(content)\n",
    "file.close()\n",
    "\n",
    "question = ['Where is Spain?'] # Example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(context = \"You did not specify any content\", questions = [\"Did you mean to specify a question?\"]):\n",
    "    \"\"\"Takes a list called 'questions' that contains the questions to answer\n",
    "Takes some text called 'content' as a source for answering questions\n",
    "Returns a dataframe of the questions with their answers and an assessment of confidence in the answers\n",
    "If no context or content is provided, returns a dataframe requesting these\"\"\"\n",
    "    \n",
    "    # List to fill with questions, answers, and confidence\n",
    "    questions_answers = []\n",
    "    \n",
    "    # For each question create an empty dictionary and call the question_answerer model on the question\n",
    "    for q in questions: \n",
    "        q_a_dict = {}\n",
    "        q_a = question_answerer(question=q, context=context)\n",
    "        \n",
    "        # Assign the question, and outputs of the question_answerer model to the dictionary\n",
    "        q_a_dict['confidence'] = q_a['score']\n",
    "        q_a_dict['question'] = q\n",
    "        q_a_dict['answer'] = q_a['answer']\n",
    "        \n",
    "        # Add the dictionary to the list and then convert the final list of dicts to a dataframe\n",
    "        questions_answers.append(q_a_dict) \n",
    "    questions_answers = pd.DataFrame(questions_answers)\n",
    "    \n",
    "    # Set a large maxcolwidth to allow for potentially long answers\n",
    "    pd.options.display.max_colwidth = 20000\n",
    "    return questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_questions(context, questions, c = 0.5, n = 5):\n",
    "    \"\"\"Selects the top n questions with the highest confidence level c\n",
    "User can define how many questions are required and the minimum confidence level\"\"\"\n",
    "    \n",
    "    # Call answer_questions to get a df of questions and answers\n",
    "    questions_answers = answer_questions(context, questions)\n",
    "    \n",
    "    # Filter for confidence\n",
    "    conf_questions = questions_answers[questions_answers['confidence'] > c] \n",
    "    \n",
    "    # Return n questions ordered by confidence\n",
    "    selected_questions = conf_questions.sort_values(by='confidence', ascending=False).head(n)\\\n",
    "    .reset_index().rename(columns={'index':'original_question_number'}) \n",
    "    \n",
    "    \n",
    "    \"\"\"Check whether enough questions can be returned and explain why if not\"\"\"\n",
    "    \n",
    "    # Were enough questions generated?\n",
    "    if len(questions_answers) < n:\n",
    "        print(f\"Only {len(questions_answers)} questions were generated\")\n",
    "        \n",
    "        # Did enough questions meet the confidence requirement?\n",
    "        if len(selected_questions) < n: \n",
    "            print(f\"Not enough questions met your required confidence level,\\\n",
    "                but here are the {len(selected_questions)} that did:\")\n",
    "        else:\n",
    "            print(f\"Here are your {n} questions\")\n",
    "            \n",
    "    else:\n",
    "        # Did enough questions meet the confidence requirement?\n",
    "        if len(selected_questions) < n: \n",
    "            print(f\"Not enough questions met your required confidence level,\\\n",
    "                but here are the {len(selected_questions)} that did:\")\n",
    "        else:\n",
    "            print(f\"Here are your {n} questions\")\n",
    "            \n",
    "    return selected_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m select_questions(\u001b[43marticle\u001b[49m, questions, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m7\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'article' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Testing function with scraped article\"\"\"\n",
    "select_questions(article, questions, 0.3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing audio as input for questions or answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Installs to analyse audio\"\"\"\n",
    "!sudo apt install ffmpeg\n",
    "!pip3 install datasets\n",
    "!pip install SoundFile\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example audio to analyse\"\"\"\n",
    "!mkdir data\n",
    "!curl https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/harvard.wav > data/harvard.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Packages for audio\"\"\"\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read the audio file and play it to verify\"\"\"\n",
    "rate, audio = wavfile.read(\"data/harvard.wav\")\n",
    "Audio(audio.T, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Transcription of a downloaded wav file\"\"\"\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa  \n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# Whisper requires a sampling rate of 16000 so must convert this with librosa\n",
    "audio, rate = librosa.load('data/harvard.wav', sr=16000)\n",
    "input_features = processor(audio, sampling_rate=rate, return_tensors=\"pt\").input_features \n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Transcription of a flac file from hugging face\"\"\"\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Processing visual input for questions or answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Final OCR extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"For \"\"\"\n",
    "!sudo apt install tesseract-ocr  \n",
    "!sudo apt install libtesseract-dev\n",
    "!pip install Pillow pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Result:\n",
      "Calculating the size of an object\n",
      "You will want to calculate the size of /\n",
      "objects under the microscope. There /\n",
      "is a simple formula for this, based\n",
      "‘on the magnification triangle.\n",
      "\n",
      "As long as you know or can measure\n",
      "two of the factors, you can find the\n",
      "third.\n",
      "\n",
      "magnification = size ohimage\n",
      "\n",
      "size of real object\n",
      "\n",
      "For example, if you know you are working at magnification x40, and\n",
      "\n",
      "the image of the cell you are looking at measures 1mm, you can\n",
      "\n",
      "work out the actual diameter of the cell:\n",
      "\n",
      "size of real object = eee Oramnage)\n",
      "magnification\n",
      "\n",
      "so\n",
      "\n",
      "=| mm=\n",
      "=49mm 0.025 mm or 25 um\n",
      "\n",
      "Your cell has a diameter of 25 um.\n",
      "\n",
      " \n",
      "\n",
      "Magnifying and resolving power\n",
      "\n",
      "Microscopes are useful because they magnify things, making them\n",
      "look bigger. The height of an average person magnified by one of the\n",
      "best light microscopes would look about 3.5km, and by an electron\n",
      "microscope about 3500 km. There is, however, a minimum distance\n",
      "between two objects when you can see them clearly as two separate\n",
      "things. If they are closer together than this, they appear as one object.\n",
      "Resolution is the ability to distinguish between two separate points\n",
      "and it is the resolving power of a microscope that affects how\n",
      "\n",
      "much detail it can show. A light microscope has a resolving power\n",
      "\n",
      "of about 200 nm, a scanning electron microscope of about 10nm\n",
      "and a transmission electron microscope of about 0.2 nm - that is\n",
      "approximately the distance apart of two atoms in a solid substance!\n",
      "\n",
      "1 Name one advantage and one disadvantage of using:\n",
      "a alight microscope [2 marks] b an electron microscope. [2 marks]\n",
      "2 a Astudent measured the diameter of a human capillary on a\n",
      "micrograph. The image measures 5 mm and the student knows\n",
      "the magnification is x1000. How many micrometres is the\n",
      "diameter of the capillary? [3 marks]\n",
      "b A student is told the image of the cell has a diameter of 800 um.\n",
      "The actual cell has a diameter of 20 um. At what magnification\n",
      "has the cell been observed? [2 marks]\n",
      "3 Evaluate the use of an electron microscope and a light microscope,\n",
      "giving one example where each type of microscope might\n",
      "\n",
      "be used. @\n",
      "\n",
      "[6 marks]\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Figure 2 Onion cells dividing as seen through\n",
      "a light microscope — magnification «570\n",
      "\n",
      " \n",
      "\n",
      "Figure 3 Chromosomes during cell division\n",
      "seen with a scanning electron microscope —\n",
      "magnification x4500\n",
      "\n",
      "You can learn more about writing ©\n",
      "very small or very large numbers in\n",
      "\n",
      "standard form in the Maths skills\n",
      "section in Topic M1b.\n",
      "\n",
      "For more information on cell division\n",
      "look at Chapter B2.\n",
      "\n",
      "ny\n",
      "\n",
      "| Make sure you can work out the\n",
      "magnification, the size of a cell, or the\n",
      "size of the image depending on the\n",
      "\n",
      "| information you are given.\n",
      "S\n",
      "\n",
      "79\n",
      "\n",
      "e Light microscopes magnify up to\n",
      "about x2000, and have a resolving\n",
      "power of about 200nm.\n",
      "\n",
      "e Electron microscopes magnify up\n",
      "to about x2000 000, and have a\n",
      "resolving power of around 0.2nm.\n",
      "\n",
      "size of image\n",
      "\n",
      "© magnification = 76 oF real object\n",
      "\n",
      "(i\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ne\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is not answering questions. It simply performs OCR on images.\n",
    "This would enable output from images to be put into the question answerer\n",
    "This should work with images obtained from the snipping tool.\n",
    "It does not recognise handwriting.\"\"\"\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def ocr_document(image_path):\n",
    "    # Open the image using the Pillow library\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Perform OCR using Tesseract\n",
    "    text = pytesseract.image_to_string(image)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/george/Downloads/magnification.jpg'\n",
    "result_text = ocr_document(image_path)\n",
    "\n",
    "print(\"OCR Result:\")\n",
    "print(result_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are your 2 questions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_question_number</th>\n",
       "      <th>confidence</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.526828</td>\n",
       "      <td>What is resolution?</td>\n",
       "      <td>the ability to distinguish between two separate points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.236706</td>\n",
       "      <td>Why are microscopes useful?</td>\n",
       "      <td>they magnify things, making them\\nlook bigger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_question_number  confidence                     question  \\\n",
       "0                         1    0.526828          What is resolution?   \n",
       "1                         0    0.236706  Why are microscopes useful?   \n",
       "\n",
       "                                                   answer  \n",
       "0  the ability to distinguish between two separate points  \n",
       "1           they magnify things, making them\\nlook bigger  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Here we use the question answering model to answer questions about the OCR text\"\"\"\n",
    "select_questions(result_text, ['Why are microscopes useful?','What is resolution?'], 0.1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### These are previous attempts at various image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example images for processing\"\"\"\n",
    "\"\"\"Text\"\"\"\n",
    "# Invoice\n",
    "invoice = 'https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png'\n",
    "# Simple poster\n",
    "simple = 'https://www.11thhourracingteam.org/wp-content/uploads/11th-hour-racing-team-how-to-create-a-sustainability-policy-horizontal-3-1-1536x1056.png'\n",
    "# Complex poster\n",
    "complicated = 'https://cdn.greenmatch.co.uk/cdn-cgi/image/format=auto/2/2023/07/MAY23_4_02-Plastic-Waste_Global-Waste_2-1-663x1024.png'\n",
    "# Microscopes text book page via web link\n",
    "microscope = 'https://m.media-amazon.com/images/I/71Ts-QXYIhL._SL1500_.jpg'\n",
    "# Magnification text book page downloaded to absolute file path\n",
    "magnification = '/home/george/Downloads/magnification.jpg'\n",
    "\n",
    "\"\"\"Handwriting\"\"\"\n",
    "# Nice clear handwriting and cursive handwriting\n",
    "clear = 'https://steemitimages.com/DQmcdbSGrnA9zeqWrYHD8EkNjvF9uxQCAeB7qnucUShpNDe/IMG_7345.PNG'\n",
    "# Tricky handwriting\n",
    "tricky = 'https://www.researchgate.net/profile/Neeta-Nain/publication/299666231/figure/fig1/AS:491693964304386@1494240384780/Example-image-of-a-general-handwritten-text-paragraph-from-IAM-dataset-4.png'\n",
    "y5 = 'https://thelinksprimary.org.uk/wp-content/uploads/2023/10/Handwriting-Y6.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### This is for reading images with text in, eg invoices or posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at impira/layoutlm-invoices were not used when initializing LayoutLMForQuestionAnswering: ['token_classifier_head.weight', 'token_classifier_head.bias']\n",
      "- This IS expected if you are initializing LayoutLMForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"First model - this answers questions about documents\n",
    "- this works for very simple documents \n",
    "but struggles for anything which implies relationships (e.g. two text boxes that relate to one another)\"\"\"\n",
    "ocr = pipeline(model = 'impira/layoutlm-invoices') #This struggles to find relationships between objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7278719544410706,\n",
       "  'answer': 'Calculating the size of an object',\n",
       "  'start': 0,\n",
       "  'end': 5}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Question-answer format\"\"\"\n",
    "ocr(image='/home/george/Downloads/magnification.jpg',question=\"What does this page say?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### This is for reading handwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This works well for single lines of handwriting but does not support multiple lines.\n",
    "I need to split multiple line files into single lines.\"\"\"\n",
    "\n",
    "hw = pipeline(model = 'microsoft/trocr-base-handwritten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved line 1 to /home/george/Downloads/split_text/line_1.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This attempts to split images. It is the first time I gave up and got chatgpt to write code for me.\n",
    "It does not work very well - it identifies words but does not link them correctly as lines.\"\"\"\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import pytesseract\n",
    "\n",
    "def split_and_save_handwritten_lines(image_path, output_directory):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Use adaptive thresholding to preprocess the image\n",
    "    _, binary_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # List to store individual line images\n",
    "    line_images = []\n",
    "\n",
    "    # Minimum width and height threshold for a contour to be considered a line\n",
    "    min_width_threshold = 300\n",
    "    min_height_threshold = 20\n",
    "\n",
    "    # Iterate through contours\n",
    "    for i, contour in enumerate(contours):\n",
    "        # Get bounding box for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Filter out contours based on width and height\n",
    "        if w > min_width_threshold and h > min_height_threshold:\n",
    "            # Crop the original image to extract the line\n",
    "            line_image = image[y:y+h, x:x+w]\n",
    "\n",
    "            # Save the line image to the output directory\n",
    "            output_path = os.path.join(output_directory, f'line_{i+1}.png')\n",
    "            cv2.imwrite(output_path, line_image)\n",
    "\n",
    "            # Append the line image to the list\n",
    "            line_images.append(line_image)\n",
    "\n",
    "    return line_images\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/george/Downloads/Handwriting-Y4.png'\n",
    "output_directory = '/home/george/Downloads/split_text'\n",
    "lines = split_and_save_handwritten_lines(image_path, output_directory)\n",
    "\n",
    "# Print the paths of saved line images\n",
    "for i, line_image in enumerate(lines, start=1):\n",
    "    print(f\"Saved line {i} to {os.path.join(output_directory, f'line_{i}.png')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"These are possible ways to better process images\"\"\"\n",
    "\"\"\"visual bert needs more configuring\"\"\"\n",
    "https://huggingface.co/daki97/visualbert_finetuned_easy_vqa\n",
    "https://huggingface.co/docs/transformers/model_doc/visual_bert#overview # overview is part of the url, not a comment\n",
    "https://github.com/huggingface/transformers/blob/main/examples/research_projects/visual_bert/demo.ipynb\n",
    "\"\"\"layout needs more configuring\"\"\"\n",
    "https://huggingface.co/docs/transformers/model_doc/layoutlmv3\n",
    "\"\"\"should work for extracting printed text, but only works for single lines\"\"\"\n",
    "https://huggingface.co/microsoft/trocr-base-printed\n",
    "\"\"\"suggestions on how to split into multiple lines\"\"\"\n",
    "https://github.com/microsoft/unilm/issues/628\n",
    "https://discuss.huggingface.co/t/trocr-fine-tuning/13293/3\n",
    "\"\"\"vision encoder requires more configuration\"\"\"\n",
    "https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder\n",
    "\"\"\"Generate LaTEX from images\"\"\"\n",
    "https://huggingface.co/Norm/nougat-latex-base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
