{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c1d459-de3e-4014-a1fe-422f039738ba",
   "metadata": {},
   "source": [
    "# Imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9547643e-da89-4b9d-a6a0-eb177d4a0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "\n",
    "datasets = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6794bb-6f0c-412b-8bbd-61c1e97a8712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded493c6-51dd-4f4e-9745-e2a2d54178bd",
   "metadata": {},
   "source": [
    "# First model - **question answering** from a given **context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff675b8-5fee-4a79-8118-46569bf12a5d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac92af6-be9e-4953-ac2c-7e61dc50c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e210a3a-3ee7-485c-a466-781e3729d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.mask_token)\n",
    "tokenizer.vocab[tokenizer.mask_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb0fcf9-b0fa-4e04-957f-faba13fe6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384  # The maximum length of a feature (question and context)\n",
    "doc_stride = (\n",
    "    128  # The authorized overlap between two part of the context when splitting\n",
    ")\n",
    "# it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bef76fe-70dc-4722-850a-f394e6756908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a\n",
    "    # stride. This results in one example possible giving several features when a context is long,\n",
    "    # each of those features having a context that overlaps a bit the context of the previous\n",
    "    # feature.\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a\n",
    "    # map from a feature to its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # The offset mappings will give us a map from token to character position in the original\n",
    "    # context. This will help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what\n",
    "        # is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this\n",
    "        # span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the\n",
    "            # CLS index).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the\n",
    "                # answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge\n",
    "                # case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets)\n",
    "                    and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73ceea1-c516-46f5-968b-97cc5cd64ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    "    num_proc=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce725fe-e9fd-4191-a548-931178e76e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tokenized_datasets[\"train\"].with_format(\"numpy\")[:]  # Load the whole dataset as a dict of numpy arrays\n",
    "validation_set = tokenized_datasets[\"validation\"].with_format(\"numpy\")[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b92ac7-327d-4d26-b34d-5972098da2c6",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc80488-5379-448f-8207-914fa2d2f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 13:39:04.158660: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-28 13:39:04.158696: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-28 13:39:04.158733: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-28 13:39:04.165886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 13:39:05.624808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.629331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.629539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.631357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.631745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.631907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.689710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.689922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.690092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-28 13:39:05.690233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6270 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-11-28 13:39:06.059754: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29dd447f-09b2-485a-93e6-015bff6471b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 13:39:07.045344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa75680-5999-4896-917a-d5ae79ee15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_batch = tf.data.Dataset.from_tensor_slices(train_set).batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd0e0387-cfef-4ed8-82fe-82e89f36be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 13:39:17.474453: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7fa000cc10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-28 13:39:17.474483: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti Laptop GPU, Compute Capability 8.6\n",
      "2023-11-28 13:39:17.478298: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-28 13:39:17.489187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-11-28 13:39:17.551075: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255/8255 [==============================] - 3437s 415ms/step - loss: 1.5631 - val_loss: 2.0252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f81831781f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set_batch, validation_data=validation_set, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c477a4ed-1d26-4ec2-8425-d2278ce4b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\"\"\"\n",
    "question = \"When did Beyonce start becoming popular?\"\n",
    "\n",
    "inputs = tokenizer([context], [question], return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "start_position = tf.argmax(outputs.start_logits, axis=1)\n",
    "end_position = tf.argmax(outputs.end_logits, axis=1)\n",
    "print(int(start_position), int(end_position[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7cf5b7b-d889-4e93-bcd2-1109dfe07bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFQuestionAnsweringModelOutput(loss=None, start_logits=<tf.Tensor: shape=(1, 72), dtype=float32, numpy=\n",
       "array([[  2.8123631,  -1.6307542,  -6.470889 ,  -9.087711 ,  -6.0963473,\n",
       "         -4.1782956,  -6.7646036,  -8.998924 ,  -5.7917676,  -7.1811223,\n",
       "         -9.8223   ,  -6.0954766,  -6.3098187,  -8.296563 ,  -3.551074 ,\n",
       "         -8.316595 ,  -8.259187 ,  -5.9869637,  -8.040161 ,  -9.498754 ,\n",
       "         -5.6094956,  -5.8215375,  -7.6993184,  -9.987848 ,  -5.341081 ,\n",
       "         -6.1518607,  -4.5341897,  -8.494854 ,  -5.598659 ,  -5.55557  ,\n",
       "         -7.650773 , -10.465519 ,  -6.018193 ,  -5.263391 ,  -9.749806 ,\n",
       "         -8.848738 ,  -7.2599864,  -9.642716 ,  -6.3330946,  -7.46044  ,\n",
       "         -8.738377 ,  -9.47532  ,  -6.2944283,  -8.441002 ,  -7.9790716,\n",
       "        -10.314072 , -10.118425 ,  -8.052443 ,  -7.9358354,  -5.5162854,\n",
       "         -9.276567 ,  -6.0980897,  -7.864852 ,  -6.2903833,  -7.7873297,\n",
       "        -10.095773 ,  -8.638689 ,  -9.574571 ,  -9.167831 ,  -6.5335083,\n",
       "         -7.0646143,  -9.802427 ,  -5.365843 ,  -7.659247 ,  -9.03162  ,\n",
       "         -8.959263 ,  -9.213363 , -10.191948 ,  -4.248611 , -10.59038  ,\n",
       "        -10.216564 ,  -8.959181 ]], dtype=float32)>, end_logits=<tf.Tensor: shape=(1, 72), dtype=float32, numpy=\n",
       "array([[  2.3418508,  -7.1737995,  -2.1131117,  -9.503686 ,  -8.946974 ,\n",
       "         -4.231819 ,  -6.425216 , -10.604285 ,  -8.515292 ,  -5.3003464,\n",
       "        -10.039296 ,  -9.267086 ,  -6.305264 ,  -6.5648174,  -8.374177 ,\n",
       "         -5.110549 ,  -9.330575 ,  -8.288181 ,  -6.295126 , -10.596588 ,\n",
       "         -8.548339 ,  -7.9166675,  -4.602286 ,  -8.806007 ,  -8.840735 ,\n",
       "         -8.194729 ,  -7.55379  ,  -9.791202 ,  -5.8295197,  -7.38903  ,\n",
       "         -4.782863 ,  -8.775269 ,  -9.617135 ,  -8.0839   ,  -9.21186  ,\n",
       "        -10.718371 ,  -9.270507 , -10.541818 ,  -8.052869 ,  -6.2483773,\n",
       "         -9.102257 , -10.742003 ,  -9.469194 ,  -8.394507 ,  -5.6938467,\n",
       "         -9.743755 , -10.113318 , -10.915657 , -10.585402 ,  -9.018401 ,\n",
       "        -10.392997 ,  -9.380557 ,  -7.7558064,  -7.0897484,  -5.4580126,\n",
       "         -8.886374 , -11.0447645, -10.808005 , -10.977511 ,  -9.786838 ,\n",
       "         -7.1906233, -10.483645 ,  -7.7754917,  -5.749944 ,  -8.042043 ,\n",
       "         -9.744872 , -10.434466 , -10.311145 ,  -9.323114 ,  -7.513654 ,\n",
       "         -9.768079 ,  -9.744887 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
